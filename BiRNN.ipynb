{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Our paper](https://arxiv.org/pdf/1910.12478.pdf) shows that Gaussian process behavior occurs for any architecture with independently sampled output layer weights, as long as their width is large.\n",
    "We explore this phenomenon in *Bidirectional RNNs* in this notebook, give reference implementations of the computation of the theoretical Bidirectional RNN kernel, and show that indeed, the empirical kernels concentrate around this theoretical kernel as the width gets large.\n",
    "\n",
    "This notebook is self-contained, and does not require reading other notebooks in this repo.\n",
    "\n",
    "Note: The Bidirectional RNN is not a part of the original paper, but is shown here as an example extending the paper. The calculations are shown in an intuitive way, but can be proven with more rigor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T04:44:39.678264Z",
     "start_time": "2019-10-30T04:44:37.186658Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T04:44:39.689143Z",
     "start_time": "2019-10-30T04:44:39.683154Z"
    }
   },
   "outputs": [],
   "source": [
    "sent1 = \"The brown fox jumps over the dog\".split()\n",
    "sent2 = \"The quick brown fox jumps over the lazy dog\".split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we load the GloVe vectors and their covariances that come with this repo.\n",
    "They were generated by [GloVe.ipynb](GloVe.ipynb) from the two sentences\n",
    "\n",
    "```The brown fox jumps over the dog.```\n",
    "\n",
    "```The quick brown fox jumps over the lazy dog.```\n",
    "\n",
    "Concretely, `exampleGloveVecs[:7]` contains the GloVe embeddings of the words from the first sentence, and `exampleGloveVecs[7:16]` contains those of the second sentence.\n",
    "\n",
    "Likewise, `exampleGloveCov` is a 16x16 matrix, such that `exampleGloveCov[:7, :7]` gives the autocovariance of the GloVe embeddings of the first sentence, `exampleGloveCov[7:, 7:]` gives that of the second sentence, and `exampleGloveCov[:7, 7:]` gives the covariance between those of the first and those of the second sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T04:44:39.706210Z",
     "start_time": "2019-10-30T04:44:39.693143Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "with open('ExampleGloVeVecs.npy', 'rb') as f:\n",
    "    exampleGloveVecs = np.load(f)\n",
    "with open('ExampleGloVeCov.npy', 'rb') as f:\n",
    "    exampleGloveCov = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Bidirectional RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bidirectional RNN with scalar output at every time step evolves like\n",
    "$$\n",
    "\\begin{align*}\n",
    "h^t_f &= W s^{t-1}_f + U x^{t}_f + b\\\\\n",
    "s^t_f &= \\phi(h^t_f)\\\\\n",
    "\n",
    "h^t_b &= W s^{t-1}_b + U x^{t}_b + b\\\\\n",
    "s^t_b &= \\phi(h^t_b)\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $x^t_f \\in \\mathbb{R}^m$ is the input at time $t$ for forward inputs\n",
    "- $x^t_b \\in \\mathbb{R}^m$ is the input at time $t$ for backward inputs\n",
    "- $h^t_f \\in \\mathbb{R}^n$ is the pre-activation at time $t$ for forward inputs\n",
    "- $h^t_b \\in \\mathbb{R}^n$ is the pre-activation at time $t$ for backward inputs\n",
    "- $s^t_f \\in \\mathbb{R}^n$ is the state of the RNN at time $t$ for forward inputs\n",
    "- $s^t_b \\in \\mathbb{R}^n$ is the state of the RNN at time $t$ for backward inputs\n",
    "- $W \\in \\mathbb{R}^{n \\times n}$ is the state-to-state weight matrix\n",
    "- $U \\in \\mathbb{R}^{n \\times m}$ is the input-to-state weight matrix\n",
    "- $b \\in \\mathbb{R}^n$ is the bias\n",
    "- $\\phi$ is the nonlinearity\n",
    "\n",
    "Simultaneously, two RNNs evolve with the same equations, but with the second using the input backwards. The two RNNs do not interact while calculating their output on each timestep independently.\n",
    "\n",
    "The final output can then be a variety of combinations of the two states of the RNNs. For this notebook, concatenation:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "        y^t &= \\langle v, \\frac{(s^t_f, s^t_b)}{\\sqrt{2n}}\\rangle\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "where \n",
    "\n",
    "- $v \\in \\mathbb{R}^{2n}$ is the readout weight\n",
    "- $y^t \\in \\mathbb{R}$ is the output at time t\n",
    "\n",
    "and the output is over $\\sqrt{2n}$ because the output width has been doubled due to concatenation.\n",
    "    \n",
    "Typically, $\\phi = \\tanh$, but here, we will set $\\phi = \\mathrm{erf}$ because $\\mathrm{erf}$ is very similar to $\\tanh$ and it gives way to closed form expressions for the infinite-width limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NETSOR Representation of Bidirectional RNN Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Our paper](https://arxiv.org/pdf/1910.12478.pdf) shows the universality of Gaussian process behavior roughly in two steps: 1) Any neural network architecture (more specifically, its computation on some inputs) can be expressed via a simple language called NETSOR, and 2) we show that the Gaussian process behavior occurs in any NETSOR program with large \"width\" by an inductive argument on the program structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The \"Infinite-Width\" Limit of NETSOR Programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian process behavior of a NETSOR program is a consequence of the NETSOR Master Theorem, which roughly prescribes the following rules for deriving the \"behavior\" of vectors in the program, when input matrices (denoted $\\mathsf{A}(n, n)$ above, of shape $n \\times n$) are sampled like $\\mathcal{N}(0, 1/n)$:\n",
    "\n",
    "- If a vector $y$ is a result of matrix multiplication $y = A x$, then $y$ can be thought of as a vector with iid Gaussian coordinates when $n \\to \\infty$.\n",
    "- If another vector $y'$ is also obtained by matrix multiplication $y' = A x'$, then each coordinate slice of $(y, y')$, i.e. $(y_\\alpha, y'_\\alpha)$ for each $\\alpha \\in [n]$, can be thought of as jointly Gaussian, iid in $\\alpha$. This joint Gaussian distribution is $$\\mathcal{N}\\left(0, \\frac{1}{n} \\begin{pmatrix}\\|x\\|^2 & \\langle x, x'\\rangle\\\\ \\langle x, x' \\rangle & \\|x'\\|^2 \\end{pmatrix}\\right).$$\n",
    "- If $y'$ is instead obtained from matrix multiplication by a different (independently sampled) matrix $B$ from $A$, $y' = B x'$, then $y$ and $y'$ can be thought of as independent.\n",
    "- Similarly, if $y'$ is an input vector, not generated by matrix multiplication, then $y$ can also be thought of as independent from $y'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note that these intuitive rules no longer hold if both a matrix $A$ and its transpose $A^\\top$ appear in a program! But in the version of NETSOR used here, this will not happen.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These rough rules will be the extent we discuss the Master Theorem here. \n",
    "See [the paper](https://arxiv.org/pdf/1910.12478.pdf) for a more formal treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory: Infinite-width Kernel of Bidirectional RNN Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we derive the formula for computing the Bidirectional RNN kernel. \n",
    "\n",
    "If you are not interested in seeing the derivation, go straight to the next code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the NETSOR Master Theorem (and as we argue intuitively below), the normalized inner products $\\langle s^t, s^r \\rangle / n$ for any two time steps $t$ and $r$ will converge almost surely to a deterministic value.\n",
    "Suppose we sample $v_\\alpha \\sim \\mathcal{N}(0, 1 / n)$.\n",
    "The outputs $y^t$ and $y^r$ will then be jointly Gaussian with covariance given by this limit.\n",
    "Thus computing the distribution of $y^t$ reduces to computing the almost sure limits of $\\langle s^t, s^r \\rangle / n$ for all pairs $t, r$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between $h^i$ and $h^j$ in a simple RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will examine a simple RNN, which will be used as part of the process of calculating the GP for a Bidirectional RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, suppose we sample $b_\\alpha \\sim \\mathcal{N}(0, 0) = 0$ and\n",
    "- $W_{\\alpha \\beta} \\sim \\mathcal{N}(0, 1 / n)$\n",
    "- $U_{\\alpha \\beta} \\sim \\mathcal{N}(0, 1 / m)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, according to the intuitive rules from the NETSOR Master Theorem we laid out in [Section 2.2](#The-\"Infinite-Width\"-Limit-of-NETSOR-Programs):\n",
    "- For any time $t$, the $\\alpha$-slice $\\{(W s^1)_\\alpha, \\ldots, (W s^t)_\\alpha\\}$ is distributed jointly as a multivariate Gaussian, iid for each $\\alpha$\n",
    "- For any time $t$, the $\\alpha$-slice $\\{(U x^1)_\\alpha, \\ldots, (U x^t)_\\alpha\\}$ is distributed jointly as a multivariate Gaussian, iid for each $\\alpha$\n",
    "- Vectors of the form $W s^t$ can be thought to be \"independent\" from vectors of the form $U x^s$\n",
    "- Consequently, for any time $t$, the $\\alpha$-slice $\\{h^1_\\alpha, \\ldots, h^t_\\alpha\\}$, as the sum of the slices $\\{(W s^1)_\\alpha, \\ldots, (W s^t)_\\alpha\\}$ and $\\{(U x^1)_\\alpha, \\ldots, (U x^t)_\\alpha\\}$, is distributed jointly as a Gaussian, iid for each $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write $\\Sigma(W s^i, W s^j)$ for the covariance between $(Ws^i)_\\alpha$ and $(Ws^j)_\\alpha$; likewise for $\\Sigma(U x^i, U x^j)$ and $\\Sigma(h^i, h^j)$.\n",
    "Then by the rules in [Section 2.2](#The-\"Infinite-Width\"-Limit-of-NETSOR-Programs), we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\Sigma(h^i, h^j)\n",
    "    &=\n",
    "        \\Sigma(W s^{i-1}, Ws^{j-1}) + \\Sigma(U x^i, U x^j)\n",
    "\\end{align*}\n",
    "$$\n",
    "because $W$ and $U$ are independently sampled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By some straightforward (rigorous) calculation, $\\Sigma(U x^i, U x^j) = \\langle x^i, x^j \\rangle/m$.\n",
    "Furthermore, by [Section 2.2](#The-\"Infinite-Width\"-Limit-of-NETSOR-Programs), roughly speaking\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\Sigma(W s^{i-1}, W s^{j-1})\n",
    "    &\\approx\n",
    "        \\langle s^{i-1}, s^{j-1} \\rangle /n\n",
    "\\end{align*}\n",
    "$$\n",
    "for large $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But since $s^{i-1} = \\phi(h^{i-1})$ and $s^{j-1} = \\phi(h^{j-1})$, and since we are thinking of $h^{i-1}$ and $h^{j-1}$ as having iid coordinates, intuitively we should have\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\langle s^{i-1}, s^{j-1} \\rangle /n\n",
    "    &=\n",
    "        \\frac 1 n \\sum_{\\alpha=1}^n s^{i-1}_\\alpha s^{j-1}_\\alpha\n",
    "        \\\\\n",
    "    &=\n",
    "        \\frac 1 n \\sum_{\\alpha=1}^n \\phi(h^{i-1}_\\alpha) \\phi(h^{j-1}_\\alpha)\n",
    "        \\\\\n",
    "    &\\approx\n",
    "        {\\mathbb{E}} \\phi(z_1)\\phi(z_2)\n",
    "\\end{align*}\n",
    "$$\n",
    "where $(z_1, z_2) \\sim \\mathcal{N}\\left( 0,\n",
    "\\Sigma|^h_{\\{i-1,j-1\\}}\n",
    "\\right)\n",
    "$ and $\\Sigma|^h_{\\{i-1,j-1\\}} = \\begin{pmatrix}\n",
    "\\Sigma(h^{i-1}, h^{i-1}) & \\Sigma(h^{i-1}, h^{j-1})\\\\\n",
    "\\Sigma(h^{j-1}, h^{i-1}) & \\Sigma(h^{j-1}, h^{j-1})\n",
    "\\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the above reasoning, turns out, can be made rigorous, and we have a recurrence relation\n",
    "$$\n",
    "\\Sigma(h^i, h^j) = {\\mathbb{E}} \\phi(z_1)\\phi(z_2) + \\langle x^i, x^j \\rangle /m\n",
    "$$\n",
    "where $(z_1, z_2) \\sim \\mathcal{N}\\left( 0,\n",
    "\\Sigma|^h_{\\{i-1,j-1\\}}\n",
    "\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\phi  = \\mathrm{erf}$, for example, then by a well-known formula\n",
    "$${\\mathbb{E}} \\phi(z_1)\\phi(z_2) = \\arcsin \\frac{\\Sigma(h^{i-1}, h^{j-1})}{\\sqrt{(\\Sigma(h^{i-1}, h^{i-1}) + 1/2)(\\Sigma(h^{j-1}, h^{j-1}) + 1/2)}}\n",
    "$$\n",
    "and the recursion above can be computed efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More generally, if we sample $W_{\\alpha \\beta} \\sim \\mathcal{N}(0, \\sigma_w^2 / n)$, $U_{\\alpha \\beta} \\sim \\mathcal{N}(0, \\sigma_u^2 / m)$, and $b_\\alpha \\sim \\mathcal{N}(0, \\sigma_b^2)$, then the recurrence can be generalized to\n",
    "$$\n",
    "\\Sigma(h^i, h^j) = \\sigma_w^2 {\\mathbb{E}} \\phi(z_1)\\phi(z_2) + \\sigma_u^2 \\langle x^i, x^j \\rangle /m + \\sigma_b^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are multiple input sequences $\\{x^{1a}, x^{2a}, \\ldots, x^{ta}, \\ldots\\}_a$ for $a = 1, \\ldots, k$, and we let $h^{ia}$ denote a state preactivation from the $a$th sequence at time $i$, then $h^{ia}$ and $h^{jb}$ will in general be correlated, and the above recursion can likewise be generalized to\n",
    "$$\n",
    "\\Sigma(h^{ia}, h^{jb}) = \\sigma_w^2 {\\mathbb{E}} \\phi(z_1)\\phi(z_2) + \\sigma_u^2 \\langle x^{ia}, x^{jb} \\rangle /m + \\sigma_b^2\n",
    "$$\n",
    "where $(z_1, z_2) \\sim \\mathcal{N}\\left(0,\n",
    "\\begin{pmatrix}\n",
    "\\Sigma(h^{i-1, a}, h^{i-1,a }) & \\Sigma(h^{i-1, a}, h^{j-1, b})\\\\\n",
    "\\Sigma(h^{j-1, b}, h^{i-1, a}) & \\Sigma(h^{j-1, b}, h^{j-1, b})\n",
    "\\end{pmatrix}\n",
    "\\right).$\n",
    "Again, this intuitive argument can be made rigorous (see our paper)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RNN Output Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, as $h^i$ can be thought to have iid coordinates, \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\langle s^{i-1}, s^{j-1} \\rangle /n\n",
    "    &=\n",
    "        \\frac 1 n \\sum_{\\alpha=1}^n s^{i-1}_\\alpha s^{j-1}_\\alpha\n",
    "        \\\\\n",
    "    &=\n",
    "        \\frac 1 n \\sum_{\\alpha=1}^n \\phi(h^{i-1}_\\alpha) \\phi(h^{j-1}_\\alpha)\n",
    "\\end{align*}\n",
    "$$\n",
    "should converge to a deterministic quantity $K_{i-1, j-1}$ (computed in the previous section) as $n \\to \\infty$.\n",
    "Then, with $v_\\alpha \\sim \\mathcal{N}(0, 1/n)$, the distribution of $\\{y^1, \\ldots, y^t\\}$ tends to the Gaussian\n",
    "$$\n",
    "\\mathcal{N}\\left(0,\n",
    "\\begin{pmatrix}\n",
    "K_{11} & \\cdots & K_{1t}\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "K_{t1} & \\cdots & K_{tt}\n",
    "\\end{pmatrix}\n",
    "\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More generally, if there are multiple input sequences $\\{x^{1a}, x^{2a}, \\ldots, x^{ta}, \\ldots\\}_a$ for $a = 1, \\ldots, k$, and we let $s^{ia}$ denote the state and let $y^{ia}$ denote the output at time $i$ from $a$th input sequence, then $\\langle s^{ia}, s^{jb} \\rangle /n$ should converge to some deterministic scalar $K^{ab}_{ij}$, and the distribution of $\\{y^{ia}\\}_{i, a}$ is jointly Gaussian, with covariance $\\mathrm{Cov}(y^{ia}, y^{jb}) = K^{ab}_{ij}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bidirectional RNN Output Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the simple RNN covariance has been established, we can examing a Bidirectional RNN. The output takes the form of\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "        y^t &= \\langle v, \\frac{(s^t_f, s^t_b)}{\\sqrt{2n}}\\rangle\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Meaning that for any two input sequences, the full GP kernel can be calculated as having a jointly Gaussian distribution with covariance $\\mathrm{Cov}(y^{ia}, y^{jb}) = K^{ab}_{ij}$ defined as\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "K^{ab}_{ij} = \\lim_{n \\to \\infty} \\mathrm{Cov}(y^{ia}, y^{jb}) = \\lim_{n \\to \\infty} \\mathrm{Cov}(\\langle v, \\frac{(s^t_f, s^t_b)}{\\sqrt{2n}}\\rangle, \\langle v, \\frac{(s^t_f, s^t_b)}{\\sqrt{2n}}\\rangle)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which, after simplifying and applying the NETSOR Master Theorem,\n",
    "\n",
    "$$\n",
    "K^{ab}_{ij} = \\sigma_v^2(\\mathbb{E}\\phi{Z^{h^{ia}_f}}\\phi{Z^{h^{jb}_f}} + \\mathbb{E}\\phi{Z^{h^{ia}_b}}\\phi{Z^{h^{jb}_b}})/2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for an arbitrary input orientation $o$ (forward or backward) the calculation of the relevant distribution $\\Sigma(h^{ia}_o, h^{jb}_o)$ remains the same as the regular RNN, with the only change being that an input $x^{1a}_f = x^{ta}_b$, $x^{2a}_f = x^{(t-1)a}_b$, and so on. Therefore,\n",
    "\n",
    "$$\n",
    "\\Sigma(h^{ia}_o, h^{jb}_o) = \\sigma_w^2 {\\mathbb{E}} \\phi(z_1)\\phi(z_2) + \\sigma_u^2 \\langle x^{ia}_o, x^{jb}_o \\rangle /m + \\sigma_b^2\n",
    "$$\n",
    "where $(z_1, z_2) \\sim \\mathcal{N}\\left(0,\n",
    "\\begin{pmatrix}\n",
    "\\Sigma(h^{i-1, a}_o, h^{i-1,a }_o) & \\Sigma(h^{i-1, a}_o, h^{j-1, b}_o)\\\\\n",
    "\\Sigma(h^{j-1, b}_o, h^{i-1, a}_o) & \\Sigma(h^{j-1, b}_o, h^{j-1, b}_o)\n",
    "\\end{pmatrix}\n",
    "\\right).$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In other words, we can find the GP of the forward input for both sequences and the GP of the backward input for both sequences independently, and then sum the two. \n",
    "\n",
    "For other output formats, this would not be the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Theoretical Kernel Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function `thbirnn` performs this computation in a vectorized way, where the input `ingram` gives the collection of $\\langle x^i, x^j \\rangle/m$ in matrix form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function `thbirnn` is also available from `BiRNNtheory.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the *V-transform* $V_\\phi$ of a function $\\phi: \\mathbb{R} \\to \\mathbb{R}$ means the function taking covariance matrices to covariance matrices by the formula\n",
    "$$V_\\phi(\\Sigma) = \\mathbb{E} \\phi(Z)\\phi(Z)^\\top, Z \\sim \\mathcal{N}(0, \\Sigma).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T04:44:39.721142Z",
     "start_time": "2019-10-30T04:44:39.709141Z"
    }
   },
   "outputs": [],
   "source": [
    "def thbirnn(ingram, inputidxs, Vphi,\n",
    "            varw=1, varu=1, varb=0, varv=1,\n",
    "            maxlength=None):\n",
    "    '''\n",
    "    Computes the infinite-width GP kernel of a bidirectional erf-RNN over input sequences\n",
    "    with normalized Gram matrix `ingram`.\n",
    "\n",
    "    A bidirectional RNN with scalar output every time step evolves like\n",
    "\n",
    "        s^t_f = nonlin(W s^{t-1}_f + U x^{t}_f + b)\n",
    "        s^t_b = nonlin(W s^{t-1}_b + U x^{t}_b + b)\n",
    "        y^t = <v, (s^t_f, s^t_b) >\n",
    "\n",
    "    where\n",
    "\n",
    "        x^t_f is the forward input at time t\n",
    "        x^t_b is the backward input at time t\n",
    "        s^t_f is the forward state of the RNN at time t\n",
    "        s^t_b is the backward state of the RNN at time t\n",
    "        W is the state-to-state weight matrix\n",
    "        U is the input-to-state weight matrix\n",
    "        b is the bias\n",
    "        v is the readout weight\n",
    "        y^t is the output at time t\n",
    "        nonlin = erf in the case of erf-RNN here\n",
    "\n",
    "    If n is the number of neurons and x^t has dimension m, and\n",
    "\n",
    "        W_{pq} ~ N(0, varw / n)\n",
    "        U_{pq} ~ N(0, varu / m)\n",
    "        b_p ~ N(0, varb)\n",
    "        v_p ~ N(0, varv / n)\n",
    "\n",
    "    then all outputs of an RNN over all time steps and all input sequences\n",
    "    are jointly Gaussian in the limit as n -> infinity.\n",
    "    This Gaussian distribution has dimension\n",
    "\n",
    "        \\sum_seq length(seq)\n",
    "\n",
    "    It has zero mean, and in this function we calculate its kernel.\n",
    "\n",
    "    Inputs:\n",
    "        `ingram`: normalized Gram matrix between all tokens across all input sequences\n",
    "        `inputidxs`: indices of `ingram` that indicate starts of input sequences\n",
    "        `Vphi`: V transform of the nonlinearity of the RNN (e.g. arcsin for step function, etc)\n",
    "        `varw`: variance of state-to-state weights\n",
    "        `varu`: variance of input-to-state weights\n",
    "        `varb`: variance of biases\n",
    "        `maxlength`: max length of all sequences. Default: None.\n",
    "            In this case, it is calculated from `inputidxs`\n",
    "    Outputs:\n",
    "        The kernel of the Gaussian distribution described above.\n",
    "    '''\n",
    "\n",
    "    ingram_b = np.zeros(ingram.shape)\n",
    "    loc = inputidxs[1]\n",
    "    ingram_b[:loc, :loc] = ingram[:loc, :loc][::-1, ::-1]\n",
    "    ingram_b[loc:, loc:] = ingram[loc:, loc:][::-1, ::-1]\n",
    "    ingram_b[:loc, loc:] = ingram[:loc, loc:][::-1, ::-1]\n",
    "    ingram_b[loc:, :loc] = ingram[loc:, :loc][::-1, ::-1]\n",
    "\n",
    "    if maxlength is None:\n",
    "        maxlength = 0\n",
    "        for i in range(len(inputidxs)-1):\n",
    "            maxlength = max(maxlength, inputidxs[i+1]-inputidxs[i])\n",
    "\n",
    "    hcov_f = np.zeros(ingram.shape)\n",
    "    hhcov_f = np.zeros(ingram.shape)\n",
    "\n",
    "    hcov_b = np.zeros(ingram.shape)\n",
    "    hhcov_b = np.zeros(ingram.shape)\n",
    "\n",
    "    for _ in range(maxlength):\n",
    "        hhcov_f[1:, 1:] = hcov_f[:-1, :-1]\n",
    "        hhcov_f[inputidxs, :] = hhcov_f[:, inputidxs] = 0\n",
    "        hhcov_f += varu * ingram + varb\n",
    "        hcov_f = varw * Vphi(hhcov_f)\n",
    "        \n",
    "        hhcov_b[1:, 1:] = hcov_b[:-1, :-1]\n",
    "        hhcov_b[inputidxs, :] = hhcov_b[:, inputidxs] = 0\n",
    "        hhcov_b += varu * ingram_b + varb\n",
    "        hcov_b = varw * Vphi(hhcov_b)\n",
    "    return varv * (hcov_f+hcov_b) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Infinite-Width Limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try computing the infinite-width kernel of a bidirectional erf-RNN with $\\sigma_w = \\sigma_u = 1, \\sigma_b = 0$, over the GloVe embeddings of the two sentences `sent1` and `sent2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T04:44:39.793170Z",
     "start_time": "2019-10-30T04:44:39.723140Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import VErf, VReLU, VStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T04:44:39.803142Z",
     "start_time": "2019-10-30T04:44:39.796142Z"
    }
   },
   "outputs": [],
   "source": [
    "varw = 1\n",
    "varu = 1\n",
    "varb = 0\n",
    "# if want to do ReLU then use `VReLU` instead of `VErf`; similarly for step function\n",
    "thcov = thbirnn(exampleGloveCov, [0, 7], VErf, varw, varu, varb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the kernel looks like, compared to the kernel of GloVe embeddings of the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T04:44:42.131704Z",
     "start_time": "2019-10-30T04:44:39.806143Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import getCor, colorbar\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.subplot(221)\n",
    "ax = plt.gca()\n",
    "im_thcov = plt.imshow(thcov, cmap='PuBu_r')\n",
    "span = np.linspace(-.5, 15.5)\n",
    "plt.plot(span, [6.5]*len(span), 'r')\n",
    "plt.plot([6.5]*len(span), span, 'r')\n",
    "plt.yticks(np.arange(16), sent1+sent2)\n",
    "plt.xticks([])\n",
    "plt.title('Bidirectional RNN covariances (theory)')\n",
    "plt.ylabel('sent2                       sent1')\n",
    "plt.grid()\n",
    "colorbar(im_thcov)\n",
    "\n",
    "plt.subplot(222)\n",
    "ax = plt.gca()\n",
    "im_thcor = plt.imshow(getCor(thcov), cmap='viridis')\n",
    "span = np.linspace(-.5, 15.5)\n",
    "plt.plot(span, [6.5]*len(span), 'r')\n",
    "plt.plot([6.5]*len(span), span, 'r')\n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "plt.title('Bidirectional RNN correlations (theory)')\n",
    "plt.grid()\n",
    "colorbar(im_thcor)\n",
    "\n",
    "\n",
    "plt.subplot(223)\n",
    "ax = plt.gca()\n",
    "im_glove = plt.imshow(exampleGloveCov, cmap='PuBu_r')\n",
    "span = np.linspace(-.5, 15.5)\n",
    "plt.plot(span, [6.5]*len(span), 'r')\n",
    "plt.plot([6.5]*len(span), span, 'r')\n",
    "plt.yticks(np.arange(16), sent1+sent2)\n",
    "plt.xticks(np.arange(16), sent1+sent2, rotation=90)\n",
    "plt.title('GloVe covariances')\n",
    "plt.xlabel('sent1                       sent2')\n",
    "plt.ylabel('sent2                       sent1')\n",
    "plt.grid()\n",
    "colorbar(im_glove)\n",
    "\n",
    "plt.subplot(224)\n",
    "ax = plt.gca()\n",
    "im_glovecor = plt.imshow(getCor(exampleGloveCov), cmap='viridis')\n",
    "span = np.linspace(-.5, 15.5)\n",
    "plt.plot(span, [6.5]*len(span), 'r')\n",
    "plt.plot([6.5]*len(span), span, 'r')\n",
    "plt.yticks([])\n",
    "plt.xticks(np.arange(16), sent1+sent2, rotation=90)\n",
    "plt.title('GloVe correlations')\n",
    "plt.xlabel('sent1                       sent2')\n",
    "plt.grid()\n",
    "colorbar(im_glovecor)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T04:44:42.153664Z",
     "start_time": "2019-10-30T04:44:42.140313Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('BiRNN.kernel', 'wb') as f:\n",
    "    np.save(f, thcov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifying Theory with Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly initialize 100 bidirectional RNNs for each width among $[2^5, 2^6, \\ldots, 2^{13}]$ and run them on the two sentences above.\n",
    "We calculate the empirical Gram matrix of the RNN embeddings (stored in `mysimcovs`) as well as its Frobenius distance to the infinite-width theoretical kernel `thcov` (stored in `frobs`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T04:44:42.179665Z",
     "start_time": "2019-10-30T04:44:42.161663Z"
    }
   },
   "outputs": [],
   "source": [
    "from BiRNNsim import simbirnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T04:48:50.591807Z",
     "start_time": "2019-10-30T04:44:42.184662Z"
    }
   },
   "outputs": [],
   "source": [
    "nsamples = 100\n",
    "widths = [2**i for i in range(5, 13)]\n",
    "mysimcovs = {}\n",
    "for width in widths:\n",
    "    mysimcovs[width] = np.array([\n",
    "        simbirnn([exampleGloveVecs[:7], exampleGloveVecs[7:]], width, F.erf, varw, varu, varb, seed=seed)[1]\n",
    "        for seed in range(nsamples)])\n",
    "frobs = []\n",
    "for width in widths:\n",
    "    _frobs = np.sum((mysimcovs[width] - thcov)**2, axis=(1, 2)) / np.linalg.norm(thcov)**2\n",
    "    for f in _frobs:\n",
    "        frobs.append(dict(\n",
    "            relfrob=f,\n",
    "            width=width\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the relative Frobenius distances in a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T04:48:50.604808Z",
     "start_time": "2019-10-30T04:48:50.594809Z"
    }
   },
   "outputs": [],
   "source": [
    "frob_df = pd.DataFrame(frobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T04:48:50.651992Z",
     "start_time": "2019-10-30T04:48:50.608998Z"
    }
   },
   "outputs": [],
   "source": [
    "frob_df.to_pickle('BiRNN.df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deviation from infinite-width theory drops with width, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T04:48:52.550987Z",
     "start_time": "2019-10-30T04:48:50.653996Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x='width', y='relfrob', data=frob_df)\n",
    "plt.semilogy()\n",
    "# plt.legend()\n",
    "plt.title('Deviation From Infinite-width Theory')\n",
    "_ = plt.ylabel(u'Relative Squared Frob. Norm\\n $\\|K_{\\infty} - K_{width}\\|_F^2/\\|K_{\\infty}\\|_F^2$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deviation from theory in *squared* Frobenius norm scales like $\\frac{1}{width}$ (so, in Frobenius norm, the deviation scales like $width^{-1/2}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T04:48:54.001041Z",
     "start_time": "2019-10-30T04:48:52.556253Z"
    }
   },
   "outputs": [],
   "source": [
    "frob_df.groupby('width', as_index=False).mean().plot.line(x='width', y='relfrob')\n",
    "plt.plot(widths, np.array(widths, dtype='float')**-1, '--', label=u'${width}^{-1}$')\n",
    "plt.ylabel(u'Mean Relative Squared Frob. Norm\\n $\\|K_{\\infty} - K_{width}\\|_F^2/\\|K_{\\infty}\\|_F^2$')\n",
    "plt.loglog()\n",
    "plt.legend()\n",
    "_ = plt.title(u'Deviation from theory in (Frobenius norm)$^2$ drops like $width^{-1}$')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('netsor': conda)",
   "language": "python",
   "name": "python38364bitnetsorconda90413646b69740418857320fb86a86c0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}